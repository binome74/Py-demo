> Необходимо:
> 
> 1. Написать скрипт на Python который загружает в БД (sqlite3) данные по каждому твиту из файла “three_minutes_tweets.json.txt”:
> структура твита: name, tweet_text, country_code, display_url, lang, created_at, location
> 
> 2. Для каждого твита в базе необходимо завести атрибут с эмоциональной оценкой сообщения: tweet_sentiment
> 
> 3. Подумать как можно нормализовать хранение твита (привести SQL-скрипты создания/изменения нормализованной структуры данных)

П.п. 1, 2 и 3 сделаны сразу:
- скрипт 00_Create_database.sql поадется на вход sqlite3.exe
```
sqlite3.exe < 00_Create_database.sql
```
- скрипт 01_Extract.py загружает данные в БД

Так как location из списка user - поле произвольного ввода, то оно для ответа на последующий запрос "вывести наиболее и наименее счастливую локацию" не подходит. Тогда берем поле name из списка place. Отсюда имеем очевидную функциональную зависимость place -> country_code, которую и выносим в отдельную таблицу (справочник). Далее, display_url может относиться к ссылкам, которые публикует пользователь, так и к ссылкам на опубликованное пользователем медиа. Это всё к твиту относится как 1:M, поэтому также выносим в отдельную дочернюю таблицу. Других зависимостей, которые бы нуждались в нормализации, на таблице твитов нет.

> 4. Написать скрипт на Python для подсчета среднего sentiment (Эмоциональной окраски сообщения) на основе AFINN-111.txt 
> и заполнить  для каждого твита tweet_sentiment колонку. 
> Если твит не содержит слов из словаря то предполагать что sentiment =0
> AFINN ReadMe:
> AFINN is a list of English words rated for valence with an integer
> between minus five (negative) and plus five (positive). The words have
> been manually labeled by Finn Arup Nielsen in 2009-2011. The file
> is tab-separated. There are two versions:
> AFINN-111: Newest version with 2477 words and phrases.

См. скрипт 02_Sentiment.py.

> 5. Написать SQL скрипт, который выводит наиболее и наименее счастливую страну, локацию и пользователя 
> (дял пользователя - вместе с его твитами), предоставить сами скрипты и результаты их работы.

См. скрипты 03_Happy-Grumpy-Loc.sql, 04_Happy-Grumpy-Country.sql, 05_Happy-Grumpy-User.sql.
Скрипт 06_CHECK_Happy-Grumpy-User.sql также является корректным решением (в разрезе пользователя), использовался для контроля корректности результатов, возвращаемых другой версией.

> 6. Описать свое видение решения, которое позволит выполнять ежедневно анализ согласно п.5. Из каких компонентов должно 
> состоять решение, из каких шагов должен состоять ETL процесс от обработки входящих файлов до этапа сохранения конечной
> информации.
> - на входе - непрерывный поток на FTP твитов в файлах (tweet.json), с частотой каждые три минуты. 
> Размеры файлов - в среднем x10 от предоставленного сэмпла.
> - на выходе - пользователи должны иметь возможность анализировать счастье по странам, локациям, пользователям, 
> отслеживать изменения, собирать статиcтику и т.д. 

Шаг 1. Скачать файлы. Предполагаем, что файлы как-то поименованы по некоторому шаблону, что по имени можно определить время их происхождения.
а) если на FTP-сервере они более не требуются, то после скачивания удалять их;
б) иначе предусмотреть, например, некоторую интеграционную таблицу в БД, в которой хранить список загруженных файлов и статус их обработки.
Некоторый процесс, поставленный на планировщик, каждые три минуты обращается к новой порции файлов и скачивает их на локальную (по отношению к скриптам) машину в зону стейджинга. Далее процесс вызывает скрипты ETL.

Шаг 2. ETL: по порядку парсить файлы и загружать их в БД, используя скрипты из задания. Имеет смысл скомбинировать скрипты 01_Extract.py и 02_Sentiment.py чтобы дважды не перечитывать исходный файл и сразу заполнять sentiment в таблице твитов, не делая дополнительный UPDATE. Если сервер не сможет умещать все структуры в памяти, то имеет смысл сделать поблочную обработку: бить входной файл на "перевариваемые" порции, которые скармливать в БД. В конце стереть или заархивировать обработанные файлы.
Для отслеживания изменений можно снабдить каждую загружаемую запись атрибутом PROCESSED_ID - некоторым порядковым номером (счётчиком) очередного запуска ETL-процесса. Зная последний обработанный им самим PROCESSED_ID клиент сможет выделять дельту.

Шаг 3. Пользователи уже имеют возможность выполнить интересующие их запросы прямо в базу для анализа любых интересующих их аспектов. Дополнительно, в зависимости от потребностей бизнеса, можно подумать о:
- создании представлений, чтобы отделить пользователей от самих таблиц с данными;
- создании дополнительных таблиц, в которые посуточно сохранять агрегаты по "счастью" (т.к. они уже не поменяются);
- выгрузке данных в некоторое BI-решение для построения отчетов.